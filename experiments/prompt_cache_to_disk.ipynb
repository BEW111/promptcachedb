{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oCBhA2Ye6gbC"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AW7ZMmUVJvC1"
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DAVn1Sdp64X8"
   },
   "outputs": [],
   "source": [
    "INITIAL_PROMPT=\"\"\"\n",
    "Prompt caching + persistent prompt db\n",
    "\n",
    "# Goal\n",
    "\n",
    "- Release a library that can be used in conjunction with any HF model, that provides the following:\n",
    "    - cache_activation(model, prompt)\n",
    "    - run_with_activation(model, cached_prompt, prompt_suffix)\n",
    "    - The cached activations should be stored in a persistent database\n",
    "- I really like one of the extensionsâ€”making a publicly available prompt cache api\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o1ApaOjL7HsO"
   },
   "outputs": [],
   "source": [
    "original_prompt_cache = DynamicCache()\n",
    "\n",
    "inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_prompt_cache = model(**inputs_initial_prompt, past_key_values=original_prompt_cache).past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2iMuUN5HKN25"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def save_cache_to_disk(prompt_cache: DynamicCache, path: str):\n",
    "    tensors = {\n",
    "        # We should investigate why the KV tensors are not contiguous\n",
    "        f\"{key_or_value}_{layer_index}\": tensor.contiguous()\n",
    "        for key_or_value, layer_index, tensor in\n",
    "        chain(\n",
    "            ((\"k\", layer_idx, tensor) for layer_idx, tensor in enumerate(prompt_cache.key_cache)),\n",
    "            ((\"v\", layer_idx, tensor) for layer_idx, tensor in enumerate(prompt_cache.value_cache))\n",
    "        )\n",
    "    }\n",
    "\n",
    "    save_file(tensors, path)\n",
    "\n",
    "\n",
    "def load_cache_from_disk(path: str) -> DynamicCache:\n",
    "    cache = DynamicCache()\n",
    "    tensors = load_file(path)\n",
    "    num_layers = len(tensors) // 2\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        key_states = tensors[f\"k_{layer_idx}\"].to(device)\n",
    "        value_states = tensors[f\"v_{layer_idx}\"].to(device)\n",
    "        cache.update(key_states, value_states, layer_idx)\n",
    "\n",
    "    return cache\n",
    "\n",
    "save_cache_to_disk(original_prompt_cache, \"prompt_cache.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K8bPoAwEOH-l"
   },
   "outputs": [],
   "source": [
    "reloaded_prompt_cache = load_cache_from_disk(\"prompt_cache.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VZ6X9Dhaqc0",
    "outputId": "cddfedbf-55c2-4e49-eeb8-3658ba14add4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCache()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_prompt_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9v02lCqVRs7",
    "outputId": "fe131e0f-bb5d-41f8-94ef-1ea0f9b50480"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Project Name\n",
      "\n",
      "\n",
      "- [PromptCache](https://github.com/alexisbregman/promptcache)\n",
      "\n",
      "# Installation\n",
      "\n",
      "- Clone\n",
      "\n",
      "# Next Steps\n",
      "\n",
      "\n",
      "- [ ] Add a prompt cache api\n",
      "- [ ] Add a prompt cache api\n",
      "- [ ] Add a prompt\n",
      "\n",
      "# Potential issues\n",
      "\n",
      "\n",
      "- The cache should be able to handle multiple prompts\n",
      "- The cache should be able to handle multiple prompts with different prompts\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"\\n# Project Name\", \"\\n# Next Steps\", \"\\n# Potential issues\"]\n",
    "responses = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    full_prompt = INITIAL_PROMPT + prompt\n",
    "    new_inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # We need to make a copy of the original cache for each prompt, since\n",
    "    # it gets modified by each of the generation runs in `prompts`\n",
    "    cache = copy.deepcopy(reloaded_prompt_cache)\n",
    "    outputs = model.generate(**new_inputs, past_key_values=cache, max_new_tokens=25)\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    responses.append(response)\n",
    "\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    full_prompt = INITIAL_PROMPT + prompt\n",
    "    print(prompt)\n",
    "    print(response[len(full_prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8s1OoQNUVX0T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
